---
title: 'IS607 Final Project: School Finances'
author: "Chris Fenton"
date: "December 13, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

My motivation for this project was to gain some insight into public education in the United States.
Specifically, I want to see if finances, whether it's revenue or expenditure, act as a predictor for school performance.

Libraries used:

```{r warning=FALSE}
library(jsonlite)
library(httr)
library(RCurl)
library(XML)
library(plyr)
library(dplyr)
library(reshape2)
```


##Acquisition

###School Financial Data

The finance data for every public school district in the USA was avaialable at the [United States Censu Bureau's website.](http://www2.census.gov/govs/school/) I began with the 2013 data, which is the most recent dataset, but wound up switching to the 2011 data, for reasons that will be elucidated on in due course. The dataset description for 2011 can be found (here.)[http://www2.census.gov/govs/school/school11doc.pdf]

The data provides an almost overwhelming level of detail in which it breaks down different revenue streams and expenditure categories. Federal, State, and Local Revenue categories are provided, with another level of detail below each. The second group of data elements are Expenditures, which include totals and detail level for categories such as instructor salaries, benefit programs, capital outlays, administration, and more. Finally, the data set
includes some ratios using the first two categories, such as % revenue from federal sources and spending per pupil.

The data is above in the above directory in a relation format, but was an excel spreadsheet. CSV files are a little
bit more predictable to read into R, so I downloaded the file, converted it to csv, and read it into R.

```{r}
fed_data <- read.csv('https://raw.githubusercontent.com/cjf4/IS-607-Data-Acquisition-and-Management/master/Final%20Project/elsec11.csv', 
                     stringsAsFactors = FALSE)
tbl_df(fed_data)
```

Simple enough. However, in the context of my desired analysis, the above served as explanatory variables. I want to 
measure performance as my response variable, so that would have to come elsewhere.

###School Performance Data

*First Attempt*

The first place I looked was the federal Department of Education. The DOE provides data in both a relational data files and APIs. An educational professional I consulted with had mentioned that 9th grade algebra scores are often used as a KPI (Key Performanc Metric) in education. [This link](https://inventory.data.gov/dataset/cee9d558-313f-41f7-8955-1d6e4df3a2dc/resource/0f696b5b-129c-4a83-93af-662f5f8162f4) appeared promising, but wound up having a couple of issues that forced me to explore other options.

The first was that there was no apparent dataset description. The above link contained a truncated "dataset abstract", but after some searching, I was not able to find the full description. While some of the columns seemed to be self evident, some were not. It would be impossible to rely on this data without a working knowledge of what
the data represented.

The second issue was with the web API. At first glance, the API appeared to be clean, simple, and allow the powerful option of including SQL queries as part of the http request. The API documentation can be [found here](https://inventory.data.gov/api/1/util/snippet/api_info.html?resource_id=89fec729-9ab9-43d5-8dcd-e65dfab2a17c&datastore_root_url=https%3A%2F%2Finventory.data.gov%2Fapi%2Faction).

However, separate attempts to access the data via all the mentioned methods proved unsuccessful, usually resulting in 409 errors (access forbidden).


I tried post a JSON object as a request, which is mentioned as an option on the API page, however this did not work.

```{r}

req <- POST('https://inventory.data.gov/api/action/datastore_search',
            body = "{
                        resource_id: 'b4e7f148-3afc-4782-84a3-dfaca3156e8b', 
                        limit: 5, 
                        q: 'jones' 
                     }",
            encode = 'json'

          
)
```

A URL with an SQL query received a 409 error as well.

```{r}
req <- POST("https://inventory.data.gov/api/action/datastore_search_sql?api_key=W7KRCyAf29uIuiklIvJaSgJliE6GrRpR01ObMUh6&sql=SELECT * from 'b4e7f148-3afc-4782-84a3-dfaca3156e8b' WHERE 'leanm11' LIKE 'jones'")
```

Suspicious, I used the SQL example:

https://inventory.data.gov/api/action/datastore_search_sql?sql=SELECT * from "89fec729-9ab9-43d5-8dcd-e65dfab2a17c" WHERE title LIKE 'jones'

mentioned in [the API page](https://inventory.data.gov/api/1/util/snippet/api_info.html?resource_id=89fec729-9ab9-43d5-8dcd-e65dfab2a17c&datastore_root_url=https%3A%2F%2Finventory.data.gov%2Fapi%2Faction), which also resulted in a 409 error. It was at that point that I decided that the API was not reliable enough for me to work with, even if it (likely) included some mistakes on my end.

*Alternate Math Performance Data Source*

After searching for another source for data that had Math peformance listed by state, I found a promising set at the National Center for Education Statistics (NCES), a sub organization of the US Department of Education. Each state had a summary performance page for a variety of subjects and grade levels. [Here is New York's page](https://nces.ed.gov/programs/stateprofiles/sresult.asp?mode=full&displaycat=7&s1=36). 

Fortunately, this page included 8th Grade Math peformance by state, and the data was from 2011 (thus explaining why I chose 2011 financial data above). Of all the subjects, Math was the most recently collected data, which was a little curious since 2011 was almost 5 years ago. C'est la vie (when it comes to beauracracies, anyway).

The other attractive thing about these web pages was that the URLs followed a recognizable pattern for each state: two digits, from 01 to 55 (some
of the intermittent two digits did not return a state, for some reason, but that did not cause a problem). What this allowed me to do was build
up a character vector of "01" to "55", which I would go on to use to construct URLs for each state's page by pasting to a "base" url.

```{r warning=FALSE}
state_ids <- as.character(rep(1:55))
state_ids <- unlist(lapply(state_ids, function(x) if(nchar(x) < 2) paste0("0", x) else x ))

url <- 'https://nces.ed.gov/programs/stateprofiles/sresult.asp?mode=full&displaycat=7&s1='
```

I started with the first state, Alabama, and worked to figure out a repeatable pattern for downloading the data from each URL. With the pattern determined, I could write functions and simply apply them to each URL.

My first function takes a two digit character vector and creates a URL and uses the getURL function to issue a HTTP get request to the server. With the request, it converts all the HTML tables to data frames using the readHTMLTable function. Finally, it pulls
the germane data frame out and returns it.

```{r warning=FALSE}
#takes a string as an argument

per_data_dl <- function(state_id) {
  state_uri <- getURL(paste0(url, state_id))
  state_table <- readHTMLTable(state_uri, stringsAsFactors = FALSE)
  state_df <- state_table[[9]]
  return(state_df)
}
```

*This part jumps ahead to tranformation a little bit, but I thought it made more sense to include in immediate succesion to the above work*

For each state's dataframe, the object returned in the above function, I would need to clean up the data a little bit so I could
get it into a coherent data frame. The state was identified as one of the column names in the data frame, so I was able to easily
obtain that. Next, I looked for all of the relevant math data, and stored each in a variable. Finally, I took all the created
variables and created a one observation data frame for each state.

```{r warning=FALSE}
data_extract <- function(df) {
  state_name <- colnames(df)[3]
  math_scale <- as.integer(df[6,3])
  math_basic <- as.integer(df[7,3])
  math_pro <- as.integer(df[8,3])
  math_adv <- as.integer(df[9,3])
  return(data.frame(state_name, math_scale, math_basic, math_pro, math_adv))
}
```

With the above two functions, I could call them on my list of state numbers created earlier.

```{r warning=FALSE}
df_list <- lapply(state_ids, per_data_dl)
extracted <- lapply(df_list, data_extract)
```

Finally, since each data frame was created with the same function, it was easy to combine them into one big data frame with the performance data. The V3's were the mysterious non specific states, which didn't provide any problems.

```{r}
math_data <- ldply(extracted, data.frame)
head(math_data)
```

##Transformations

```{r}
#sum pivot
finance_sum <- group_by(fed_data, STATE)
summed_fin <- summarise(finance_sum, sum_rev = sum(TOTALREV), sum_exp = sum(TOTALEXP), total_enroll = sum(V33))

#read in state key
state_key <- read.csv('https://raw.githubusercontent.com/cjf4/IS-607-Data-Acquisition-and-Management/master/Final%20Project/state_key.csv', stringsAsFactors = FALSE)

#attach state name to summarized data
school_finance_per <- merge(summed_fin, state_key, by.x = "STATE", by.y = "state_id")

#attach sum data to performance data
school_finance_per <- merge(school_finance_per, math_data, by = "state_name")

#create dollars per pupil for each state
school_finance_per$exp_per_pup <- school_finance_per$sum_exp / school_finance_per$total_enroll

```