---
title: 'IS607 Final Project: School Finances'
author: "Chris Fenton"
date: "December 13, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

My motivation for this project was to gain some insight into public education in the United States.
Specifically, I want to see if finances, whether it's revenue or expenditure, act as a predictor for school performance.

Libraries used:

```{r warning=FALSE, message=FALSE}
library(jsonlite)
library(httr)
library(RCurl)
library(XML)
library(plyr)
library(dplyr)
library(reshape2)
```


##Acquisition

###School Financial Data

The finance data for every public school district in the USA was avaialable at the [United States Censu Bureau's website.](http://www2.census.gov/govs/school/) I began with the 2013 data, which is the most recent dataset, but wound up switching to the 2011 data, for reasons that will be elucidated on in due course. The dataset description for 2011 can be found (here.)[http://www2.census.gov/govs/school/school11doc.pdf]

The data provides an almost overwhelming level of detail in which it breaks down different revenue streams and expenditure categories. Federal, State, and Local Revenue categories are provided, with another level of detail below each. The second group of data elements are Expenditures, which include totals and detail level for categories such as instructor salaries, benefit programs, capital outlays, administration, and more. Finally, the data set
includes some ratios using the first two categories, such as % revenue from federal sources and spending per pupil.

The data is above in the above directory in a relation format, but was an excel spreadsheet. CSV files are a little
bit more predictable to read into R, so I downloaded the file, converted it to csv, and read it into R.

```{r}
fed_data <- read.csv('https://raw.githubusercontent.com/cjf4/IS-607-Data-Acquisition-and-Management/master/Final%20Project/elsec11.csv', 
                     stringsAsFactors = FALSE)
tbl_df(fed_data)
```

Simple enough. However, in the context of my desired analysis, the above served as explanatory variables. I want to 
measure performance as my response variable, so that would have to come elsewhere.

###School Performance Data

*First Attempt*

The first place I looked was the federal Department of Education. The DOE provides data in both a relational data files and APIs. An educational professional I consulted with had mentioned that 9th grade algebra scores are often used as a KPI (Key Performanc Metric) in education. [This link](https://inventory.data.gov/dataset/cee9d558-313f-41f7-8955-1d6e4df3a2dc/resource/0f696b5b-129c-4a83-93af-662f5f8162f4) appeared promising, but wound up having a couple of issues that forced me to explore other options.

The first was that there was no apparent dataset description. The above link contained a truncated "dataset abstract", but after some searching, I was not able to find the full description. While some of the columns seemed to be self evident, some were not. It would be impossible to rely on this data without a working knowledge of what
the data represented.

The second issue was with the web API. At first glance, the API appeared to be clean, simple, and allow the powerful option of including SQL queries as part of the http request. The API documentation can be [found here](https://inventory.data.gov/api/1/util/snippet/api_info.html?resource_id=89fec729-9ab9-43d5-8dcd-e65dfab2a17c&datastore_root_url=https%3A%2F%2Finventory.data.gov%2Fapi%2Faction).

However, separate attempts to access the data via all the mentioned methods proved unsuccessful, usually resulting in 409 errors (access forbidden).


I tried post a JSON object as a request, which is mentioned as an option on the API page, however this did not work.

```{r}

req <- POST('https://inventory.data.gov/api/action/datastore_search',
            body = "{
                        resource_id: 'b4e7f148-3afc-4782-84a3-dfaca3156e8b', 
                        limit: 5, 
                        q: 'jones' 
                     }",
            encode = 'json'

          
)
```

A URL with an SQL query received a 409 error as well.

```{r}
req <- POST("https://inventory.data.gov/api/action/datastore_search_sql?api_key=W7KRCyAf29uIuiklIvJaSgJliE6GrRpR01ObMUh6&sql=SELECT * from 'b4e7f148-3afc-4782-84a3-dfaca3156e8b' WHERE 'leanm11' LIKE 'jones'")
```

Suspicious, I used the SQL example:

https://inventory.data.gov/api/action/datastore_search_sql?sql=SELECT * from "89fec729-9ab9-43d5-8dcd-e65dfab2a17c" WHERE title LIKE 'jones'

mentioned in [the API page](https://inventory.data.gov/api/1/util/snippet/api_info.html?resource_id=89fec729-9ab9-43d5-8dcd-e65dfab2a17c&datastore_root_url=https%3A%2F%2Finventory.data.gov%2Fapi%2Faction), which also resulted in a 409 error. It was at that point that I decided that the API was not reliable enough for me to work with, even if it (likely) included some mistakes on my end.

*Alternate Math Performance Data Source*

After searching for another source for data that had Math peformance listed by state, I found a promising set at the National Center for Education Statistics (NCES), a sub organization of the US Department of Education. Each state had a summary performance page for a variety of subjects and grade levels. [Here is New York's page](https://nces.ed.gov/programs/stateprofiles/sresult.asp?mode=full&displaycat=7&s1=36). 

Fortunately, this page included 8th Grade Math peformance by state, and the data was from 2011 (thus explaining why I chose 2011 financial data above). Of all the subjects, Math was the most recently collected data, which was a little curious since 2011 was almost 5 years ago. C'est la vie (when it comes to beauracracies, anyway).

The other attractive thing about these web pages was that the URLs followed a recognizable pattern for each state: two digits, from 01 to 55 (some
of the intermittent two digits did not return a state, for some reason, but that did not cause a problem). What this allowed me to do was build
up a character vector of "01" to "55", which I would go on to use to construct URLs for each state's page by pasting to a "base" url.

```{r warning=FALSE}
state_ids <- as.character(rep(1:55))
state_ids <- unlist(lapply(state_ids, function(x) if(nchar(x) < 2) paste0("0", x) else x ))

url <- 'https://nces.ed.gov/programs/stateprofiles/sresult.asp?mode=full&displaycat=7&s1='
```

I started with the first state, Alabama, and worked to figure out a repeatable pattern for downloading the data from each URL. With the pattern determined, I could write functions and simply apply them to each URL.

My first function takes a two digit character vector and creates a URL and uses the getURL function to issue a HTTP get request to the server. With the request, it converts all the HTML tables to data frames using the readHTMLTable function. Finally, it pulls
the germane data frame out and returns it.

```{r warning=FALSE}
#takes a string as an argument

per_data_dl <- function(state_id) {
  state_uri <- getURL(paste0(url, state_id))
  state_table <- readHTMLTable(state_uri, stringsAsFactors = FALSE)
  state_df <- state_table[[9]]
  return(state_df)
}
```

*This part jumps ahead to tranformation a little bit, but I thought it made more sense to include in immediate succesion to the above work*

For each state's dataframe, the object returned in the above function, I would need to clean up the data a little bit so I could
get it into a coherent data frame. The state was identified as one of the column names in the data frame, so I was able to easily
obtain that. Next, I looked for all of the relevant math data, and stored each in a variable. Finally, I took all the created
variables and created a one observation data frame for each state.

```{r warning=FALSE}
data_extract <- function(df) {
  state_name <- colnames(df)[3]
  math_scale <- as.integer(df[6,3])
  math_basic <- as.integer(df[7,3])
  math_pro <- as.integer(df[8,3])
  math_adv <- as.integer(df[9,3])
  return(data.frame(state_name, math_scale, math_basic, math_pro, math_adv))
}
```

With the above two functions, I could call them on my list of state numbers created earlier.

```{r warning=FALSE}
df_list <- lapply(state_ids, per_data_dl)
extracted <- lapply(df_list, data_extract)
```

Finally, since each data frame was created with the same function, it was easy to combine them into one big data frame with the performance data. The V3's were the mysterious non specific states, which didn't provide any problems.

```{r}
math_data <- ldply(extracted, data.frame)
head(math_data)
```

##Transformations

Now that I had everything (or almost everything, as will become apparent) in R, I was reading to starting getting my data in a workable format.

The first task was to summarize the finance data. Each observation in the finance data set was a district, but I am comparing states. So, I needed to sum up each variable that I cared about by state. Using the group_by and summarise functions from dplyr,
this was painless. I decided to just look at the total revenue, total expenditures to keep things simple. I wasn't able to use the provided ratios, since they were by district. This would mean I would be taking the average of an average and not weighting appropriately. Instead, I summed the total enrollment figure for each each district, giving me the total enrollment for each state.

```{r}
finance_sum <- group_by(fed_data, STATE)
summed_fin <- summarise(finance_sum, sum_rev = sum(TOTALREV), sum_exp = sum(TOTALEXP), total_enroll = sum(V33))
```

However at this point it became apparent that the State variable were not state names, but instead state codes from the [data definition file]([http://www2.census.gov/govs/school/school11doc.pdf]). Using the file, I created .csv file and read it in to R.

```{r}
state_key <- read.csv('https://raw.githubusercontent.com/cjf4/IS-607-Data-Acquisition-and-Management/master/Final%20Project/state_key.csv', stringsAsFactors = FALSE)
```

This data frame served as a key of sorts, and allowed me to attach the name to each summed state observation in the finance data frame. I used the merge function for this task.
```{r}
school_finance_per <- merge(summed_fin, state_key, by.x = "STATE", by.y = "state_id")
```

Again, using the merege function, I was finally able to combine by financial data with my performance data.
```{r}
school_finance_per <- merge(school_finance_per, math_data, by = "state_name")
```

Finally, since the absolute revenue/spending difference between states of different populations would be difficult to compare, I created revenue and expenditure per pupil variables.
```{r message=FALSE, warning=FALSE}
school_finance_per$exp_per_pup <- school_finance_per$sum_exp / school_finance_per$total_enroll
school_finance_per$rev_per_pup <- school_finance_per$sum_rev / school_finance_per$total_enroll
head(school_finance_per)
```

##Analysis

I'll start by taking a look at some easy to calculate statistics. First up, do schools on aggregate stay on budget?

```{r}
str(subset(school_finance_per, exp_per_pup > rev_per_pup))
```
So we can see 29 of the 51 states (plus DC) ran deficits in aggreagate in 2011.

Another quick qestion would be which stats spend the most per pupil on education?

```{r}
head(top_spenders <- school_finance_per[order(school_finance_per$exp_per_pup, decreasing = TRUE),])
```

And the least?

```{r}
tail(top_spenders <- school_finance_per[order(school_finance_per$exp_per_pup, decreasing = TRUE),])
```

Now it's time to look at the relationship between spending and math performance. First, we will look at expenditure per pupil against basic math proficiency, which is defined as "implies partial mastery of prerequisite knowledge and skills that are fundamental for proficient work at each grade."

```{r}
plot(school_finance_per$exp_per_pup, school_finance_per$math_basic, xlab = "Expenditure per pupil", ylab = "Basic Math Profiency Percentage")
```

We can see a clear linear trend of correlation between expenditure and Basic Math proficiency. However, of interest is the outlier on this graph, that spends more per pupil than any other state/district, yet achieves the worst performance, both by wide margins. From the above listing of highest spending states, we know this is Washington DC.

Since their appears to be a linear relationship, I will try to fit a regression line using the two sets of data.

```{r}
basic_math.lm <- lm(math_basic ~ exp_per_pup, school_finance_per)
summary(basic_math.lm)
```

Interesting. The regression line actually predicts a *negative* relationship between the two variables. Washington DC must be throwing off the model.

I'll try again, only this time exclude Washington DC.

```{r}
schools_no_DC <- subset(school_finance_per, math_basic > 50)
basic_math.lm <- lm(math_basic ~ exp_per_pup, schools_no_DC)
summary(basic_math.lm)
```

That gives us an positive relationship. For every additional 1,000 spent per pupil, we would predict a state's percentage of students who achieve basic profiecieny to increase by .87. However, the R-squared on this model is quite low, and this is not establishing a causal relationship, so it would be imprudent to advise states to start pouring money into their education budgets.

The curious case of Washington DC illustrates a need for futher analysis. No other state spend so much, yet get so little for their money. This is likely related to the fact that Washington DC is exclusively an urban district. Other inner city districts that may be otherwise comparable to Washington would have their results mixed in with rural and suburban districts. If nothing else, this illustrates the need to study inner city districts separately from the rest of the country.